{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "d6556329",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow import keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "c94d3458",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "tf.enable_eager_execution must be called at program startup.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[52], line 6\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mmatplotlib\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpyplot\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mplt\u001b[39;00m\n\u001b[0;32m      5\u001b[0m \u001b[38;5;66;03m#from tensorflow.examples.tutorials.mnist import input_data\u001b[39;00m\n\u001b[1;32m----> 6\u001b[0m \u001b[43mtf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcompat\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mv1\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43menable_eager_execution\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py:6155\u001b[0m, in \u001b[0;36menable_eager_execution\u001b[1;34m(config, device_policy, execution_mode)\u001b[0m\n\u001b[0;32m   6153\u001b[0m logging\u001b[38;5;241m.\u001b[39mvlog(\u001b[38;5;241m1\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEnabling eager execution\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m   6154\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m context\u001b[38;5;241m.\u001b[39mdefault_execution_mode \u001b[38;5;241m!=\u001b[39m context\u001b[38;5;241m.\u001b[39mEAGER_MODE:\n\u001b[1;32m-> 6155\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43menable_eager_execution_internal\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   6156\u001b[0m \u001b[43m      \u001b[49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   6157\u001b[0m \u001b[43m      \u001b[49m\u001b[43mdevice_policy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdevice_policy\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   6158\u001b[0m \u001b[43m      \u001b[49m\u001b[43mexecution_mode\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mexecution_mode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   6159\u001b[0m \u001b[43m      \u001b[49m\u001b[43mserver_def\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py:6223\u001b[0m, in \u001b[0;36menable_eager_execution_internal\u001b[1;34m(config, device_policy, execution_mode, server_def)\u001b[0m\n\u001b[0;32m   6220\u001b[0m   graph_mode_has_been_used \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m   6221\u001b[0m       _default_graph_stack\u001b[38;5;241m.\u001b[39m_global_default_graph \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m)  \u001b[38;5;66;03m# pylint: disable=protected-access\u001b[39;00m\n\u001b[0;32m   6222\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m graph_mode_has_been_used:\n\u001b[1;32m-> 6223\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m   6224\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtf.enable_eager_execution must be called at program startup.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m   6225\u001b[0m context\u001b[38;5;241m.\u001b[39mdefault_execution_mode \u001b[38;5;241m=\u001b[39m context\u001b[38;5;241m.\u001b[39mEAGER_MODE\n\u001b[0;32m   6226\u001b[0m \u001b[38;5;66;03m# pylint: disable=protected-access\u001b[39;00m\n",
      "\u001b[1;31mValueError\u001b[0m: tf.enable_eager_execution must be called at program startup."
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import tensorflow.compat.v1 as tf\n",
    "import tensorflow_datasets as tfds\n",
    "import matplotlib.pyplot as plt\n",
    "#from tensorflow.examples.tutorials.mnist import input_data\n",
    "tf.compat.v1.enable_eager_execution()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "50f0828b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "too many values to unpack (expected 2)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[64], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m (x_train, y_train), (x_test, y_test) \u001b[38;5;241m=\u001b[39m tfds\u001b[38;5;241m.\u001b[39mload(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mfashion_mnist\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "\u001b[1;31mValueError\u001b[0m: too many values to unpack (expected 2)"
     ]
    }
   ],
   "source": [
    "(x_train, y_train), (x_test, y_test) = tfds.load('fashion_mnist')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "6f7a0f2e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/train-labels-idx1-ubyte.gz\n",
      "29515/29515 [==============================] - 0s 2us/step\n",
      "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/train-images-idx3-ubyte.gz\n",
      "26421880/26421880 [==============================] - 7s 0us/step\n",
      "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/t10k-labels-idx1-ubyte.gz\n",
      "5148/5148 [==============================] - 0s 0s/step\n",
      "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/t10k-images-idx3-ubyte.gz\n",
      "4422102/4422102 [==============================] - 1s 0us/step\n"
     ]
    }
   ],
   "source": [
    "(x_train, y_train), (x_test, y_test) =tf.keras.datasets.fashion_mnist.load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "95e2d8ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train2 = tf.keras.datasets.fashion_mnist.load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "0bfeffc8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((array([[[0, 0, 0, ..., 0, 0, 0],\n",
       "          [0, 0, 0, ..., 0, 0, 0],\n",
       "          [0, 0, 0, ..., 0, 0, 0],\n",
       "          ...,\n",
       "          [0, 0, 0, ..., 0, 0, 0],\n",
       "          [0, 0, 0, ..., 0, 0, 0],\n",
       "          [0, 0, 0, ..., 0, 0, 0]],\n",
       "  \n",
       "         [[0, 0, 0, ..., 0, 0, 0],\n",
       "          [0, 0, 0, ..., 0, 0, 0],\n",
       "          [0, 0, 0, ..., 0, 0, 0],\n",
       "          ...,\n",
       "          [0, 0, 0, ..., 0, 0, 0],\n",
       "          [0, 0, 0, ..., 0, 0, 0],\n",
       "          [0, 0, 0, ..., 0, 0, 0]],\n",
       "  \n",
       "         [[0, 0, 0, ..., 0, 0, 0],\n",
       "          [0, 0, 0, ..., 0, 0, 0],\n",
       "          [0, 0, 0, ..., 0, 0, 0],\n",
       "          ...,\n",
       "          [0, 0, 0, ..., 0, 0, 0],\n",
       "          [0, 0, 0, ..., 0, 0, 0],\n",
       "          [0, 0, 0, ..., 0, 0, 0]],\n",
       "  \n",
       "         ...,\n",
       "  \n",
       "         [[0, 0, 0, ..., 0, 0, 0],\n",
       "          [0, 0, 0, ..., 0, 0, 0],\n",
       "          [0, 0, 0, ..., 0, 0, 0],\n",
       "          ...,\n",
       "          [0, 0, 0, ..., 0, 0, 0],\n",
       "          [0, 0, 0, ..., 0, 0, 0],\n",
       "          [0, 0, 0, ..., 0, 0, 0]],\n",
       "  \n",
       "         [[0, 0, 0, ..., 0, 0, 0],\n",
       "          [0, 0, 0, ..., 0, 0, 0],\n",
       "          [0, 0, 0, ..., 0, 0, 0],\n",
       "          ...,\n",
       "          [0, 0, 0, ..., 0, 0, 0],\n",
       "          [0, 0, 0, ..., 0, 0, 0],\n",
       "          [0, 0, 0, ..., 0, 0, 0]],\n",
       "  \n",
       "         [[0, 0, 0, ..., 0, 0, 0],\n",
       "          [0, 0, 0, ..., 0, 0, 0],\n",
       "          [0, 0, 0, ..., 0, 0, 0],\n",
       "          ...,\n",
       "          [0, 0, 0, ..., 0, 0, 0],\n",
       "          [0, 0, 0, ..., 0, 0, 0],\n",
       "          [0, 0, 0, ..., 0, 0, 0]]], dtype=uint8),\n",
       "  array([9, 0, 0, ..., 3, 0, 5], dtype=uint8)),\n",
       " (array([[[0, 0, 0, ..., 0, 0, 0],\n",
       "          [0, 0, 0, ..., 0, 0, 0],\n",
       "          [0, 0, 0, ..., 0, 0, 0],\n",
       "          ...,\n",
       "          [0, 0, 0, ..., 0, 0, 0],\n",
       "          [0, 0, 0, ..., 0, 0, 0],\n",
       "          [0, 0, 0, ..., 0, 0, 0]],\n",
       "  \n",
       "         [[0, 0, 0, ..., 0, 0, 0],\n",
       "          [0, 0, 0, ..., 0, 0, 0],\n",
       "          [0, 0, 0, ..., 0, 0, 0],\n",
       "          ...,\n",
       "          [0, 0, 0, ..., 0, 0, 0],\n",
       "          [0, 0, 0, ..., 0, 0, 0],\n",
       "          [0, 0, 0, ..., 0, 0, 0]],\n",
       "  \n",
       "         [[0, 0, 0, ..., 0, 0, 0],\n",
       "          [0, 0, 0, ..., 0, 0, 0],\n",
       "          [0, 0, 0, ..., 0, 0, 0],\n",
       "          ...,\n",
       "          [0, 0, 0, ..., 0, 0, 0],\n",
       "          [0, 0, 0, ..., 0, 0, 0],\n",
       "          [0, 0, 0, ..., 0, 0, 0]],\n",
       "  \n",
       "         ...,\n",
       "  \n",
       "         [[0, 0, 0, ..., 0, 0, 0],\n",
       "          [0, 0, 0, ..., 0, 0, 0],\n",
       "          [0, 0, 0, ..., 0, 0, 0],\n",
       "          ...,\n",
       "          [0, 0, 0, ..., 0, 0, 0],\n",
       "          [0, 0, 0, ..., 0, 0, 0],\n",
       "          [0, 0, 0, ..., 0, 0, 0]],\n",
       "  \n",
       "         [[0, 0, 0, ..., 0, 0, 0],\n",
       "          [0, 0, 0, ..., 0, 0, 0],\n",
       "          [0, 0, 0, ..., 0, 0, 0],\n",
       "          ...,\n",
       "          [0, 0, 0, ..., 0, 0, 0],\n",
       "          [0, 0, 0, ..., 0, 0, 0],\n",
       "          [0, 0, 0, ..., 0, 0, 0]],\n",
       "  \n",
       "         [[0, 0, 0, ..., 0, 0, 0],\n",
       "          [0, 0, 0, ..., 0, 0, 0],\n",
       "          [0, 0, 0, ..., 0, 0, 0],\n",
       "          ...,\n",
       "          [0, 0, 0, ..., 0, 0, 0],\n",
       "          [0, 0, 0, ..., 0, 0, 0],\n",
       "          [0, 0, 0, ..., 0, 0, 0]]], dtype=uint8),\n",
       "  array([9, 2, 1, ..., 8, 1, 5], dtype=uint8)))"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "c55a4cee",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Training parameters\n",
    "lr = 0.0002\n",
    "batch_size = 128\n",
    "epochs = 10000\n",
    "\n",
    "#Network Parameters\n",
    "img_dim = 784\n",
    "gen_hid_dim = 256\n",
    "disc_hid_dim = 256\n",
    "z_noise_dim = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "272691be",
   "metadata": {},
   "outputs": [],
   "source": [
    "def xavier_init(shape):\n",
    "    return tf.random.normal(shape, 1/tf.sqrt(shape[0]/2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "02a5d012",
   "metadata": {},
   "outputs": [],
   "source": [
    "#weight and biases\n",
    "weights = {'disc_H': tf.Variable(xavier_init([img_dim, disc_hid_dim]), dtype=tf.float32),\n",
    "         'disc_final': tf.Variable(xavier_init([disc_hid_dim, 1]), dtype=tf.float32),\n",
    "         'gen_H': tf.Variable(xavier_init([z_noise_dim, gen_hid_dim]), dtype=tf.float32),\n",
    "          'gen_final': tf.Variable(xavier_init([gen_hid_dim, img_dim]), dtype=tf.float32)\n",
    "         }\n",
    "\n",
    "bias = {\n",
    "        'disc_H': tf.Variable(xavier_init([disc_hid_dim]), dtype=tf.float32),\n",
    "         'disc_final': tf.Variable(xavier_init([1]), dtype=tf.float32),\n",
    "         'gen_H': tf.Variable(xavier_init([gen_hid_dim]), dtype=tf.float32),\n",
    "          'gen_final': tf.Variable(xavier_init([img_dim]), dtype=tf.float32)\n",
    "    }\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "cdea7ad4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#discriminator\n",
    "def disc(x):\n",
    "    hid_layer = tf.nn.relu(tf.add(tf.matmul(x, weights['disc_H']), bias['disc_H']))\n",
    "    final_layer = tf.add(tf.matmul(hid_layer, weights['disc_final']), bias['disc_final'])\n",
    "    disc_output = tf.nn.sigmoid(final_layer)\n",
    "    return final_layer, disc_output\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "7b3200de",
   "metadata": {},
   "outputs": [],
   "source": [
    "#generator\n",
    "def gen(x):\n",
    "    hidden_layer = tf.nn.relu(tf.add(tf.matmul(x, weights['gen_H']), bias['gen_H']))\n",
    "    final_layer = tf.add(tf.matmul(hidden_layer, weights['gen_final']), bias['gen_final'])\n",
    "    gen_output = tf.sigmoid(final_layer)\n",
    "    return gen_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "c6247df1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#tf.compat.v1.disable_eager_execution()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "d0beec35",
   "metadata": {},
   "outputs": [],
   "source": [
    "#place holder for external input\n",
    "z_input = tf.compat.v1.placeholder(tf.float32, shape= [None, z_noise_dim], name = 'input_noise')\n",
    "x_input = tf.compat.v1.placeholder(tf.float32, shape= [None, img_dim], name= 'real_input')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "072f4268",
   "metadata": {},
   "outputs": [],
   "source": [
    "#buld generator network\n",
    "with tf.name_scope('Generator') as scope:\n",
    "    output_Gen = gen(z_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "90cc792f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#buildin of discriminator\n",
    "with tf.name_scope('Discriminator') as scope:\n",
    "    real_output_disc, real_output_disc = disc(x_input)\n",
    "    fake_output_disc, fake_output_disc = disc(output_Gen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "4c4efa6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.name_scope('Discriminator_loss') as scope:\n",
    "    Discriminator_loss = -tf.reduce_mean(tf.log(real_output_disc + 0.0001) + tf.log(1 -fake_output_disc+0.0001))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "14d1ead6",
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.name_scope('Generator_loss'):\n",
    "    Generator_loss = -tf.reduce_mean(tf.log(fake_output_disc+0.0001))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "1c11d626",
   "metadata": {},
   "outputs": [],
   "source": [
    "disc_loss_total = tf.summary.scalar('Disc_Total_Loss', Discriminator_loss)\n",
    "gen_loss_total = tf.summary.scalar('Gen_Loss', Generator_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "17e6d442",
   "metadata": {},
   "outputs": [],
   "source": [
    "#variables\n",
    "Generator_var = [weights['gen_H'], weights['gen_final'], bias['gen_H'], bias['gen_final']]\n",
    "Discriminator_var = [weights['disc_H'], weights['disc_final'], bias['disc_H'], bias['disc_final']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "d6b43818",
   "metadata": {},
   "outputs": [],
   "source": [
    "#dfn optimizer\n",
    "with tf.name_scope('Optimizer_Discriminator') as scope:\n",
    "    Discriminator_optimize = tf.train.AdamOptimizer(learning_rate = lr).minimize(Discriminator_loss, var_list = Discriminator_var)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "e8794be9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#dfn optimizer\n",
    "with tf.name_scope('Optimizer_Discriminator') as scope:\n",
    "    Generator_optimize = tf.train.AdamOptimizer(learning_rate = lr).minimize(Discriminator_loss, var_list = Discriminator_var)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "06db0f68",
   "metadata": {},
   "outputs": [],
   "source": [
    "init = tf.global_variables_initializer()\n",
    "sess = tf.Session()\n",
    "sess.run(init)\n",
    "writer = tf.summary.FileWriter('./log', sess.graph)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "d99260c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\morara\\AppData\\Local\\Temp\\ipykernel_11300\\2371929557.py:2: batch (from tensorflow.python.training.input) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Queue-based input pipelines have been replaced by `tf.data`. Use `tf.data.Dataset.batch(batch_size)` (or `padded_batch(...)` if `dynamic_pad=True`).\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\morara\\AppData\\Local\\Temp\\ipykernel_11300\\2371929557.py:2: batch (from tensorflow.python.training.input) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Queue-based input pipelines have been replaced by `tf.data`. Use `tf.data.Dataset.batch(batch_size)` (or `padded_batch(...)` if `dynamic_pad=True`).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\morara\\anaconda3\\lib\\site-packages\\tensorflow\\python\\training\\input.py:749: QueueRunner.__init__ (from tensorflow.python.training.queue_runner_impl) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "To construct input pipelines, use the `tf.data` module.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\morara\\anaconda3\\lib\\site-packages\\tensorflow\\python\\training\\input.py:749: QueueRunner.__init__ (from tensorflow.python.training.queue_runner_impl) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "To construct input pipelines, use the `tf.data` module.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\morara\\anaconda3\\lib\\site-packages\\tensorflow\\python\\training\\input.py:749: add_queue_runner (from tensorflow.python.training.queue_runner_impl) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "To construct input pipelines, use the `tf.data` module.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\morara\\anaconda3\\lib\\site-packages\\tensorflow\\python\\training\\input.py:749: add_queue_runner (from tensorflow.python.training.queue_runner_impl) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "To construct input pipelines, use the `tf.data` module.\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "The value of a feed cannot be a tf.Tensor object. Acceptable feed values include Python scalars, strings, lists, numpy ndarrays, or TensorHandles. For reference, the tensor object was Tensor(\"batch:0\", shape=(128, 60000, 28, 28), dtype=uint8) which was passed to the argument `feed_dict` with key Tensor(\"real_input_1:0\", shape=(None, 784), dtype=float32).",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[66], line 6\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;66;03m#generate noise to feed to the neural network\u001b[39;00m\n\u001b[0;32m      5\u001b[0m z_noise \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mrandom\u001b[38;5;241m.\u001b[39muniform(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m1\u001b[39m, size\u001b[38;5;241m=\u001b[39m[batch_size, z_noise_dim])\n\u001b[1;32m----> 6\u001b[0m _, Disc_loss_epoch \u001b[38;5;241m=\u001b[39m \u001b[43msess\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[43mDiscriminator_optimize\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mDiscriminator_loss\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfeed_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m{\u001b[49m\u001b[43mx_input\u001b[49m\u001b[43m:\u001b[49m\u001b[43mx_batch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mz_input\u001b[49m\u001b[43m:\u001b[49m\u001b[43mz_noise\u001b[49m\u001b[43m}\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      7\u001b[0m _, Gen_loss_epoch \u001b[38;5;241m=\u001b[39m sess\u001b[38;5;241m.\u001b[39mrun([Generator_optimize, Generator_loss], feed_dict\u001b[38;5;241m=\u001b[39m{z_input:z_noise})\n\u001b[0;32m      9\u001b[0m \u001b[38;5;66;03m#running discriminator summary\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py:968\u001b[0m, in \u001b[0;36mBaseSession.run\u001b[1;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m    965\u001b[0m run_metadata_ptr \u001b[38;5;241m=\u001b[39m tf_session\u001b[38;5;241m.\u001b[39mTF_NewBuffer() \u001b[38;5;28;01mif\u001b[39;00m run_metadata \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    967\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 968\u001b[0m   result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_run\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfetches\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfeed_dict\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptions_ptr\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    969\u001b[0m \u001b[43m                     \u001b[49m\u001b[43mrun_metadata_ptr\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    970\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m run_metadata:\n\u001b[0;32m    971\u001b[0m     proto_data \u001b[38;5;241m=\u001b[39m tf_session\u001b[38;5;241m.\u001b[39mTF_GetBuffer(run_metadata_ptr)\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py:1139\u001b[0m, in \u001b[0;36mBaseSession._run\u001b[1;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m   1135\u001b[0m   \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\n\u001b[0;32m   1136\u001b[0m       \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mCannot interpret feed_dict key as Tensor: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00me\u001b[38;5;241m.\u001b[39margs[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m   1138\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(subfeed_val, ops\u001b[38;5;241m.\u001b[39mTensor):\n\u001b[1;32m-> 1139\u001b[0m   \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\n\u001b[0;32m   1140\u001b[0m       \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mThe value of a feed cannot be a tf.Tensor object. Acceptable \u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m   1141\u001b[0m       \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mfeed values include Python scalars, strings, lists, numpy \u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m   1142\u001b[0m       \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mndarrays, or TensorHandles. For reference, the tensor object \u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m   1143\u001b[0m       \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mwas \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mstr\u001b[39m(feed_val)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m which was passed to the argument \u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m   1144\u001b[0m       \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m`feed_dict` with key \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mstr\u001b[39m(feed)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m   1146\u001b[0m subfeed_dtype \u001b[38;5;241m=\u001b[39m subfeed_t\u001b[38;5;241m.\u001b[39mdtype\u001b[38;5;241m.\u001b[39mas_numpy_dtype\n\u001b[0;32m   1147\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(subfeed_val, \u001b[38;5;28mint\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m _convert_to_numpy_obj(\n\u001b[0;32m   1148\u001b[0m     subfeed_dtype, subfeed_val) \u001b[38;5;241m!=\u001b[39m subfeed_val:\n",
      "\u001b[1;31mTypeError\u001b[0m: The value of a feed cannot be a tf.Tensor object. Acceptable feed values include Python scalars, strings, lists, numpy ndarrays, or TensorHandles. For reference, the tensor object was Tensor(\"batch:0\", shape=(128, 60000, 28, 28), dtype=uint8) which was passed to the argument `feed_dict` with key Tensor(\"real_input_1:0\", shape=(None, 784), dtype=float32)."
     ]
    }
   ],
   "source": [
    "for epoch in range(epochs):\n",
    "    x_batch, _=tf.train.batch([x_train, y_train], batch_size=batch_size)\n",
    "    \n",
    "    #generate noise to feed to the neural network\n",
    "    z_noise = np.random.uniform(-1, 1, size=[batch_size, z_noise_dim])\n",
    "    _, Disc_loss_epoch = sess.run([Discriminator_optimize, Discriminator_loss], feed_dict={x_input:x_batch, z_input:z_noise})\n",
    "    _, Gen_loss_epoch = sess.run([Generator_optimize, Generator_loss], feed_dict={z_input:z_noise})\n",
    "    \n",
    "    #running discriminator summary\n",
    "    summary_Disc_Loss = sess.run(disc_loss_total, feed_dict={x_input:x_batch, z_input:z_noise})\n",
    "    #adding discriminator summary\n",
    "    writer.add_summary(summary_Disc_Loss, epoch)\n",
    "    \n",
    "    #run generator summary\n",
    "    summary_Gen_Loss = sess.run(gen_loss_total, feed_dict={z_input:z_noise})\n",
    "    #adding generator summary\n",
    "    writer.add_summary(summary_Gen_Loss)\n",
    "    \n",
    "    if epoch% 2000 ==0:\n",
    "        print(f'steps :{epoch} : Generator Loss : {Gen_loss_epoch}, Discriminator : {Disc_loss_epoch}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6dd58bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "n = 6\n",
    "canvas = np.empty((28*n, 28*n))\n",
    "for i in range(n):\n",
    "    #noise input\n",
    "    z_noise = np.random.uniform(-1, 1, size= [batch_size, z_noise_dim])\n",
    "    #generator image from noise\n",
    "    g = sess.run(output_Gen, feed_dict{z_input:z_noise})\n",
    "    #reverse colors for better display\n",
    "    g = -1*(g-1)\n",
    "    for j in range(n):\n",
    "        #draw the generated digits\n",
    "        canvas[i*28:(i+1)*28, j*28: (j+1)*28] = g[j].reshape([28, 28])\n",
    "\n",
    "plt.figure(figsize=(n, n))\n",
    "plt.imshow(canvas, origin='upper', cmap='gray')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "d3fdeb36",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "({'train': <PrefetchDataset element_spec={'image': TensorSpec(shape=(28, 28, 1), dtype=tf.uint8, name=None), 'label': TensorSpec(shape=(), dtype=tf.int64, name=None)}>,\n",
       "  'test': <PrefetchDataset element_spec={'image': TensorSpec(shape=(28, 28, 1), dtype=tf.uint8, name=None), 'label': TensorSpec(shape=(), dtype=tf.int64, name=None)}>},\n",
       " tfds.core.DatasetInfo(\n",
       "     name='fashion_mnist',\n",
       "     full_name='fashion_mnist/3.0.1',\n",
       "     description=\"\"\"\n",
       "     Fashion-MNIST is a dataset of Zalando's article images consisting of a training set of 60,000 examples and a test set of 10,000 examples. Each example is a 28x28 grayscale image, associated with a label from 10 classes.\n",
       "     \"\"\",\n",
       "     homepage='https://github.com/zalandoresearch/fashion-mnist',\n",
       "     data_path='C:\\\\Users\\\\morara\\\\tensorflow_datasets\\\\fashion_mnist\\\\3.0.1',\n",
       "     file_format=tfrecord,\n",
       "     download_size=29.45 MiB,\n",
       "     dataset_size=36.42 MiB,\n",
       "     features=FeaturesDict({\n",
       "         'image': Image(shape=(28, 28, 1), dtype=uint8),\n",
       "         'label': ClassLabel(shape=(), dtype=int64, num_classes=10),\n",
       "     }),\n",
       "     supervised_keys=('image', 'label'),\n",
       "     disable_shuffling=False,\n",
       "     splits={\n",
       "         'test': <SplitInfo num_examples=10000, num_shards=1>,\n",
       "         'train': <SplitInfo num_examples=60000, num_shards=1>,\n",
       "     },\n",
       "     citation=\"\"\"@article{DBLP:journals/corr/abs-1708-07747,\n",
       "       author    = {Han Xiao and\n",
       "                    Kashif Rasul and\n",
       "                    Roland Vollgraf},\n",
       "       title     = {Fashion-MNIST: a Novel Image Dataset for Benchmarking Machine Learning\n",
       "                    Algorithms},\n",
       "       journal   = {CoRR},\n",
       "       volume    = {abs/1708.07747},\n",
       "       year      = {2017},\n",
       "       url       = {http://arxiv.org/abs/1708.07747},\n",
       "       archivePrefix = {arXiv},\n",
       "       eprint    = {1708.07747},\n",
       "       timestamp = {Mon, 13 Aug 2018 16:47:27 +0200},\n",
       "       biburl    = {https://dblp.org/rec/bib/journals/corr/abs-1708-07747},\n",
       "       bibsource = {dblp computer science bibliography, https://dblp.org}\n",
       "     }\"\"\",\n",
       " ))"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f_mnist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "f37e638d",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = [i for i in range(40)]\n",
    "data = np.array(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "715d9384",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9],\n",
       "        [10, 11, 12, 13, 14, 15, 16, 17, 18, 19],\n",
       "        [20, 21, 22, 23, 24, 25, 26, 27, 28, 29],\n",
       "        [30, 31, 32, 33, 34, 35, 36, 37, 38, 39]]])"
      ]
     },
     "execution_count": 118,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = data.reshape(-1, 4, 10, 1)\n",
    "data"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
